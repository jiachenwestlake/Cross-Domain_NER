# config file for train cross ner model

# supervised learning #
supervised_ner_1_train=data/conll03NER/train
supervised_ner_1_dev=data/conll03NER/dev
supervised_ner_1_test=data/conll03NER/test

supervised_ner_2_train=data/BioNLP13CG/train
supervised_ner_2_dev=data/BioNLP13CG/dev
supervised_ner_2_test=data/BioNLP13CG/test

supervised_lm_1_train=data/news.lm
supervised_lm_2_train=data/bio.lm

# transfer learning #
transfer_ner_1_train=data/conll03NER/train
transfer_ner_1_dev=data/conll03NER/dev
transfer_ner_1_test=data/conll03NER/test

transfer_ner_2_dev=data/news_tech/tech.test
transfer_ner_2_test=data/news_tech/tech.test

transfer_lm_1_train=data/news.lm
transfer_lm_2_train=data/tech.lm

# check point #
model_dir=data/check_point/cross_ner_model
init_dir=data/check_point/cross_ner_init

# word embed vocab:
word_embed_dir=data/vocab/glove.6B.100d.txt

# normalize
norm_word_emb=False
norm_char_emb=False
number_normalized=True

# network parameter #
MAX_SENTENCE_LENGTH=128
MAX_WORD_LENGTH=16

seg=True
word_emb_dim=100
char_emb_dim=30
task_emb_dim=8
domain_emb_dim=8

cnn_layer=4
char_hidden_dim=50
hidden_dim=200
dropout=0.5
lstm_layer=1
bilstm=True

use_ner_crf=True
use_char=True

char_seq_feature=CNN

# training parameter #
status=train
mode=transfer
optimizer=SGD #RMSPROP
iteration=100
batch_size=10 #30
ave_batch_loss=True

learning_rate=0.05 #0.001
learning_rate_cpg=0.05 #0.001
lr_decay=0.05
momentum=0
l2=1e-8
gpu=False
